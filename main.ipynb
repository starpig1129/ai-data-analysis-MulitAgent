{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from logger import setup_logger\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API keys and environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "WORKING_DIRECTORY = os.getenv('WORKING_DIRECTORY', './data_storage/')\n",
    "\n",
    "# Validate critical environment variables\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "if not LANGCHAIN_API_KEY:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-Agent Data Analysis System\"\n",
    "\n",
    "# Set up logger\n",
    "logger = setup_logger()\n",
    "\n",
    "# Initialize language models\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=4096)\n",
    "    power_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5, max_tokens=4096)\n",
    "    json_llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    "        temperature=0,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    logger.info(\"Language models initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing language models: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Ensure working directory exists\n",
    "if not os.path.exists(WORKING_DIRECTORY):\n",
    "    os.makedirs(WORKING_DIRECTORY)\n",
    "    logger.info(f\"Created working directory: {WORKING_DIRECTORY}\")\n",
    "\n",
    "logger.info(\"Initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state import State\n",
    "from node import agent_node,human_choice_node,note_agent_node,human_review_node,refiner_node\n",
    "from create_agent import create_agent,create_supervisor\n",
    "from router import QualityReview_router,hypothesis_router,process_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state graph for the workflow\n",
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Hypothesis\",\"Process\",\"Visualization\", \"Search\", \"Coder\", \"Report\", \"QualityReview\",\"Refiner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.internet import google_search,FireCrawl_scrape_webpages\n",
    "from tools.basetool import execute_code,execute_command\n",
    "from tools.FileEdit import create_document,read_document,edit_document,collect_data\n",
    "from langchain.agents import load_tools\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "hypothesis_agent = create_agent(\n",
    "llm, \n",
    "[collect_data,wikipedia,google_search,FireCrawl_scrape_webpages]+load_tools([\"arxiv\"],),\n",
    "'''\n",
    "As an esteemed expert in data analysis, your task is to formulate a set of research hypotheses and outline the steps to be taken based on the information table provided. Utilize statistics, machine learning, deep learning, and artificial intelligence in developing these hypotheses. Your hypotheses should be precise, achievable, professional, and innovative. To ensure the feasibility and uniqueness of your hypotheses, thoroughly investigate relevant information. For each hypothesis, include ample references to support your claims.\n",
    "\n",
    "Upon analyzing the information table, you are required to:\n",
    "\n",
    "1. Formulate research hypotheses that leverage statistics, machine learning, deep learning, and AI techniques.\n",
    "2. Outline the steps involved in testing these hypotheses.\n",
    "3. Verify the feasibility and uniqueness of each hypothesis through a comprehensive literature review.\n",
    "\n",
    "At the conclusion of your analysis, present the complete research hypotheses, elaborate on their uniqueness and feasibility, and provide relevant references to support your assertions. Please answer in structured way to enhance readability.\n",
    "Just answer a research hypothesis.\n",
    "''',\n",
    "members,WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_agent = create_supervisor(\n",
    "    power_llm,\n",
    "    \"\"\"\n",
    "    You are a research supervisor responsible for overseeing and coordinating a comprehensive data analysis project, resulting in a complete and cohesive research report. Your primary tasks include:\n",
    "\n",
    "    1. Validating and refining the research hypothesis to ensure it is clear, specific, and testable.\n",
    "    2. Orchestrating a thorough data analysis process, with all code well-documented and reproducible.\n",
    "    3. Compiling and refining a research report that includes:\n",
    "        - Introduction\n",
    "        - Hypothesis\n",
    "        - Methodology\n",
    "        - Results, accompanied by relevant visualizations\n",
    "        - Discussion\n",
    "        - Conclusion\n",
    "        - References\n",
    "\n",
    "    **Step-by-Step Process:**\n",
    "    1. **Planning:** Define clear objectives and expected outcomes for each phase of the project.\n",
    "    2. **Task Assignment:** Assign specific tasks to the appropriate agents (\"Visualization,\" \"Search,\" \"Coder,\" \"Report\").\n",
    "    3. **Review and Integration:** Critically review and integrate outputs from each agent, ensuring consistency, quality, and relevance.\n",
    "    4. **Feedback:** Provide feedback and further instructions as needed to refine outputs.\n",
    "    5. **Final Compilation:** Ensure all components are logically connected and meet high academic standards.\n",
    "\n",
    "    **Agent Guidelines:**\n",
    "    - **Visualization Agent:** Develop and explain data visualizations that effectively communicate key findings.\n",
    "    - **Search Agent:** Collect and summarize relevant information, and compile a comprehensive list of references.\n",
    "    - **Coder Agent:** Write and document efficient Python code for data analysis, ensuring that the code is clean and reproducible.\n",
    "    - **Report Agent:** Draft, refine, and finalize the research report, integrating inputs from all agents and ensuring the narrative is clear and cohesive.\n",
    "\n",
    "    **Workflow:**\n",
    "    1. Plan the overall analysis and reporting process.\n",
    "    2. Assign tasks to the appropriate agents and oversee their progress.\n",
    "    3. Continuously review and integrate the outputs from each agent, ensuring that each contributes effectively to the final report.\n",
    "    4. Adjust the analysis and reporting process based on emerging results and insights.\n",
    "    5. Compile the final report, ensuring all sections are complete and well-integrated.\n",
    "\n",
    "    **Completion Criteria:**\n",
    "    Respond with \"FINISH\" only when:\n",
    "    1. The hypothesis has been thoroughly tested and validated.\n",
    "    2. The data analysis is complete, with all code documented and reproducible.\n",
    "    3. All required visualizations have been created, properly labeled, and explained.\n",
    "    4. The research report is comprehensive, logically structured, and includes all necessary sections.\n",
    "    5. The reference list is complete and accurately cited.\n",
    "    6. All components are cohesively integrated into a polished final report.\n",
    "\n",
    "    Ensure that the final report delivers a clear, insightful analysis, addressing all aspects of the hypothesis and meeting the highest academic standards.\n",
    "    \"\"\",\n",
    "    [\"Visualization\", \"Search\", \"Coder\", \"Report\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_agent = create_agent(\n",
    "    llm, \n",
    "    [read_document, execute_code, execute_command],\n",
    "    \"\"\"\n",
    "    You are a data visualization expert tasked with creating insightful visual representations of data. Your primary responsibilities include:\n",
    "    \n",
    "    1. Designing appropriate visualizations that clearly communicate data trends and patterns.\n",
    "    2. Selecting the most suitable chart types (e.g., bar charts, scatter plots, heatmaps) for different data types and analytical purposes.\n",
    "    3. Providing executable Python code (using libraries such as matplotlib, seaborn, or plotly) that generates these visualizations.\n",
    "    4. Including well-defined titles, axis labels, legends, and saving the visualizations as files.\n",
    "    5. Offering brief but clear interpretations of the visual findings.\n",
    "\n",
    "    **File Saving Guidelines:**\n",
    "    - Save all visualizations as files with descriptive and meaningful filenames.\n",
    "    - Ensure filenames are structured to easily identify the content (e.g., 'sales_trends_2024.png' for a sales trend chart).\n",
    "    - Confirm that the saved files are organized in the working directory, making them easy for other agents to locate and use.\n",
    "\n",
    "    **Constraints:**\n",
    "    - Focus solely on visualization tasks; do not perform data analysis or preprocessing.\n",
    "    - Ensure all visual elements are suitable for the target audience, with attention to color schemes and design principles.\n",
    "    - Avoid over-complicating visualizations; aim for clarity and simplicity.\n",
    "    \"\"\",\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "\n",
    "code_agent = create_agent(\n",
    "    power_llm,\n",
    "    [read_document,execute_code, execute_command],\n",
    "    \"\"\"\n",
    "    You are an expert Python programmer specializing in data processing and analysis. Your main responsibilities include:\n",
    "\n",
    "    1. Writing clean, efficient Python code for data manipulation, cleaning, and transformation.\n",
    "    2. Implementing statistical methods and machine learning algorithms as needed.\n",
    "    3. Debugging and optimizing existing code for performance improvements.\n",
    "    4. Adhering to PEP 8 standards and ensuring code readability with meaningful variable and function names.\n",
    "\n",
    "    Constraints:\n",
    "    - Focus solely on data processing tasks; do not generate visualizations or write non-Python code.\n",
    "    - Provide only valid, executable Python code, including necessary comments for complex logic.\n",
    "    - Avoid unnecessary complexity; prioritize readability and efficiency.\n",
    "    \"\"\",\n",
    "    members,WORKING_DIRECTORY\n",
    ")\n",
    "\n",
    "searcher_agent= create_agent(\n",
    "    llm,\n",
    "    [read_document, collect_data,wikipedia,google_search,FireCrawl_scrape_webpages]+load_tools([\"arxiv\"],),\n",
    "    \"\"\"\n",
    "    You are a skilled research assistant responsible for gathering and summarizing relevant information. Your main tasks include:\n",
    "\n",
    "    1. Conducting thorough literature reviews using academic databases and reputable online sources.\n",
    "    2. Summarizing key findings in a clear, concise manner.\n",
    "    3. Providing citations for all sources, prioritizing peer-reviewed and academically reputable materials.\n",
    "\n",
    "    Constraints:\n",
    "    - Focus exclusively on information retrieval and summarization; do not engage in data analysis or processing.\n",
    "    - Present information in an organized format, with clear attributions to sources.\n",
    "    - Evaluate the credibility of sources and prioritize high-quality, reliable information.\n",
    "    \"\"\",\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "\n",
    "report_agent = create_agent(\n",
    "    power_llm, \n",
    "    [create_document, read_document, edit_document], \n",
    "    \"\"\"\n",
    "    You are an experienced scientific writer tasked with drafting comprehensive research reports. Your primary duties include:\n",
    "\n",
    "    1. Clearly stating the research hypothesis and objectives in the introduction.\n",
    "    2. Detailing the methodology used, including data collection and analysis techniques.\n",
    "    3. Structuring the report into coherent sections (e.g., Introduction, Methodology, Results, Discussion, Conclusion).\n",
    "    4. Synthesizing information from various sources into a unified narrative.\n",
    "    5. Integrating relevant data visualizations and ensuring they are appropriately referenced and explained.\n",
    "\n",
    "    Constraints:\n",
    "    - Focus solely on report writing; do not perform data analysis or create visualizations.\n",
    "    - Maintain an objective, academic tone throughout the report.\n",
    "    - Cite all sources using APA style and ensure that all findings are supported by evidence.\n",
    "    \"\"\",\n",
    "    members,WORKING_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_review_agent=create_agent(\n",
    "    llm, \n",
    "    [create_document,read_document,edit_document], \n",
    "    '''\n",
    "    You are a meticulous quality control expert responsible for reviewing and ensuring the high standard of all research outputs. Your tasks include:\n",
    "\n",
    "    1. Critically evaluating the content, methodology, and conclusions of research reports.\n",
    "    2. Checking for consistency, accuracy, and clarity in all documents.\n",
    "    3. Identifying areas that need improvement or further elaboration.\n",
    "    4. Ensuring adherence to scientific writing standards and ethical guidelines.\n",
    "\n",
    "    After your review, if revisions are needed, respond with 'REVISION' as a prefix, set needs_revision=True, and provide specific feedback on parts that need improvement. If no revisions are necessary, respond with 'CONTINUE' as a prefix and set needs_revision=False.\n",
    "    ''',\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_agent import create_note_agent\n",
    "note_agent=create_note_agent(\n",
    "    json_llm, \n",
    "    [read_document], \n",
    "    '''\n",
    "    You are a meticulous research process note-taker. Your main responsibility is to observe, summarize, and document the actions and findings of the research team. Your tasks include:\n",
    "\n",
    "    1. Observing and recording key activities, decisions, and discussions among team members.\n",
    "    2. Summarizing complex information into clear, concise, and accurate notes.\n",
    "    3. Organizing notes in a structured format that ensures easy retrieval and reference.\n",
    "    4. Highlighting significant insights, breakthroughs, challenges, or any deviations from the research plan.\n",
    "    5. Responding only in JSON format to ensure structured documentation.\n",
    "\n",
    "    Your output should be well-organized and easy to integrate with other project documentation.\n",
    "    ''',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner_agent = create_agent(\n",
    "    power_llm,  \n",
    "    [read_document, edit_document,create_document,collect_data,wikipedia,google_search,FireCrawl_scrape_webpages]+load_tools([\"arxiv\"],),\n",
    "    '''\n",
    "    You are an expert AI report refiner tasked with optimizing and enhancing research reports. Your responsibilities include:\n",
    "\n",
    "    1. Thoroughly reviewing the entire research report, focusing on content, structure, and readability.\n",
    "    2. Identifying and emphasizing key findings, insights, and conclusions.\n",
    "    3. Restructuring the report to improve clarity, coherence, and logical flow.\n",
    "    4. Ensuring that all sections are well-integrated and support the primary research hypothesis.\n",
    "    5. Condensing redundant or repetitive content while preserving essential details.\n",
    "    6. Enhancing the overall readability, ensuring the report is engaging and impactful.\n",
    "\n",
    "    Refinement Guidelines:\n",
    "    - Maintain the scientific accuracy and integrity of the original content.\n",
    "    - Ensure all critical points from the original report are preserved and clearly articulated.\n",
    "    - Improve the logical progression of ideas and arguments.\n",
    "    - Highlight the most significant results and their implications for the research hypothesis.\n",
    "    - Ensure that the refined report aligns with the initial research objectives and hypothesis.\n",
    "\n",
    "    After refining the report, submit it for final human review, ensuring it is ready for publication or presentation.\n",
    "    ''',\n",
    "    members,  \n",
    "    WORKING_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"Hypothesis\", lambda state: agent_node(state, hypothesis_agent, \"hypothesis_agent\"))\n",
    "workflow.add_node(\"Process\", lambda state: agent_node(state, process_agent, \"process_agent\"))\n",
    "workflow.add_node(\"Visualization\", lambda state: agent_node(state, visualization_agent, \"visualization_agent\"))\n",
    "workflow.add_node(\"Search\", lambda state: agent_node(state, searcher_agent, \"searcher_agent\"))\n",
    "workflow.add_node(\"Coder\", lambda state: agent_node(state, code_agent, \"code_agent\"))\n",
    "workflow.add_node(\"Report\", lambda state: agent_node(state, report_agent, \"report_agent\"))\n",
    "workflow.add_node(\"QualityReview\", lambda state: agent_node(state, quality_review_agent, \"quality_review_agent\"))\n",
    "workflow.add_node(\"NoteTaker\", lambda state: note_agent_node(state, note_agent, \"note_agent\"))\n",
    "workflow.add_node(\"HumanChoice\", human_choice_node)\n",
    "workflow.add_node(\"HumanReview\", human_review_node)\n",
    "workflow.add_node(\"Refiner\", lambda state: refiner_node(state, refiner_agent, \"refiner_agent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START\n",
    "\n",
    "workflow.add_edge(\"Hypothesis\", \"HumanChoice\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"HumanChoice\",\n",
    "    hypothesis_router,\n",
    "    {\n",
    "        \"Hypothesis\": \"Hypothesis\",\n",
    "        \"Process\": \"Process\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Process\",\n",
    "    process_router,\n",
    "    {\n",
    "        \"Coder\": \"Coder\",\n",
    "        \"Search\": \"Search\",\n",
    "        \"Visualization\": \"Visualization\",\n",
    "        \"Report\": \"Report\",\n",
    "        \"Process\": \"Process\",\n",
    "        \"Refiner\": \"Refiner\",\n",
    "    }\n",
    ")\n",
    "\n",
    "for member in [\"Visualization\",'Search','Coder','Report']:\n",
    "    workflow.add_edge(member, \"QualityReview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"QualityReview\",\n",
    "    QualityReview_router,\n",
    "    {\n",
    "        'Visualization': \"Visualization\",\n",
    "        'Search': \"Search\",\n",
    "        'Coder': \"Coder\",\n",
    "        'Report': \"Report\",\n",
    "        'NoteTaker': \"NoteTaker\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"NoteTaker\", \"Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an edge from HumanReview to Process\n",
    "workflow.add_conditional_edges(\n",
    "    \"HumanReview\",\n",
    "    lambda state: \"Process\" if state and state.get(\"needs_revision\", False) else END,\n",
    "    {\n",
    "        \"Process\": \"Process\",\n",
    "        \"END\": END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"Refiner\", \"HumanReview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "workflow.add_edge(START, \"Hypothesis\")\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInput = '''\n",
    "datapath:OnlineSalesData.csv\n",
    "Use machine learning to perform data analysis and write complete graphical reports\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=userInput\n",
    "            ),\n",
    "        ],\n",
    "        \"hypothesis\": \"\",\n",
    "        \"process_decision\":\"\",\n",
    "        \"process\": \"\",\n",
    "        \"visualization_state\": \"\",\n",
    "        \"searcher_state\": \"\",\n",
    "        \"code_state\": \"\",\n",
    "        \"report_section\": \"\",\n",
    "        \"quality_review\": \"\",\n",
    "        \"needs_revision\": False,\n",
    "        \"last_sender\": \"\",\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}, \"recursion_limit\": 3000},\n",
    "    stream_mode=\"values\",\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message,end='',flush=True)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stream(events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
